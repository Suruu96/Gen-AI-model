{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOI7d9Mid098bYi7Q8Yvb2t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hYWmEy91uJrX","executionInfo":{"status":"ok","timestamp":1724132386250,"user_tz":-330,"elapsed":484027,"user":{"displayName":"Suvarnaa","userId":"08198153561635951457"}},"outputId":"aec58873-bedf-4927-ace2-1f5c80295ff5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='46' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [46/46 06:55, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Generated Text:\n","Once upon a time when the United States was in the midst of a civil war, the Confederate States were the most powerful and powerful in the country, and were the most powerful in the world. The Confederate States were the most powerful in the world, and were the most powerful in the world in the war. The Confederate States were the most powerful in the world in the war, and the most powerful in the world in the war. The Confederate States were the most powerful in the world in the war\n","Generated Text with Loaded Model:\n","Once upon a time, the city was a bustling metropolis, with many merchants and merchants, and many merchants and merchants of all kinds. The city was a bustling metropolis, with many merchants and merchants of all kinds. The city was a bustling metropolis, with many merchants and merchants of all kinds. The city was a bustling metropolis, with many merchants and merchants of all kinds. The city was a bustling metropolis, with many merchants and merchants of all kinds. The city was a\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_9ae55c14-b6dc-49cd-8254-0449a33d7f05\", \"trained_model.zip\", 462709228)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_5186efb5-1782-4f87-b8d5-8c30bfe8d2de\", \"trained_tokenizer.zip\", 533563)"]},"metadata":{}}],"source":["!pip install transformers datasets torch\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n","from datasets import load_dataset\n","import torch\n","\n","# Load the GPT-2 model and tokenizer\n","model_name = \"gpt2\"\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","\n","# Set pad_token to eos_token\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Load a small subset of the dataset (1% of the data)\n","dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")  # Use only 1% of the dataset\n","\n","# Tokenize the dataset and set labels\n","def tokenize_function(examples):\n","    tokenized_text = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)  # Shorten max_length\n","    tokenized_text[\"labels\"] = tokenized_text[\"input_ids\"].copy()  # Use input_ids as labels\n","    return tokenized_text\n","\n","# Apply the tokenization\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","\n","# Define the training arguments with fewer epochs and smaller batch size\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=1,  # Train for only 1 epoch\n","    per_device_train_batch_size=8,  # Keep the batch size small to fit in memory\n","    save_steps=10_000,  # Save checkpoints less frequently\n","    save_total_limit=2,\n","    prediction_loss_only=False,  # Set to False to ensure loss is calculated\n","    logging_dir=\"./logs\",  # Add logging directory\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n",")\n","\n","# Train the model (this should take around 10 minutes)\n","trainer.train()\n","\n","# Example prompt\n","prompt = \"Once upon a time\"\n","\n","# Encode the prompt\n","input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n","\n","# Generate text with additional parameters to avoid warnings\n","output = model.generate(\n","    input_ids,\n","    max_length=100,\n","    num_return_sequences=1,\n","    pad_token_id=tokenizer.pad_token_id,  # Ensure padding token ID is set\n","    attention_mask=input_ids.ne(tokenizer.pad_token_id).long()  # Set attention mask to avoid warnings\n",")\n","\n","# Decode and print the generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(\"Generated Text:\")\n","print(generated_text)\n","\n","\n","model.save_pretrained(\"./trained_model\")\n","tokenizer.save_pretrained(\"./trained_tokenizer\")\n","\n","# Load the model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(\"./trained_model\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"./trained_tokenizer\")\n","\n","# Generate text again using the loaded model\n","input_ids = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\")\n","output = model.generate(\n","    input_ids,\n","    max_length=100,\n","    num_return_sequences=1,\n","    pad_token_id=tokenizer.pad_token_id,  # Ensure padding token ID is set\n","    attention_mask=input_ids.ne(tokenizer.pad_token_id).long()  # Set attention mask to avoid warnings\n",")\n","print(\"Generated Text with Loaded Model:\")\n","print(tokenizer.decode(output[0], skip_special_tokens=True))\n","\n","import os\n","\n","import shutil\n","\n","# Zip the trained_model directory\n","shutil.make_archive(\"trained_model\", 'zip', \"trained_model\")\n","\n","# Zip the trained_tokenizer directory\n","shutil.make_archive(\"trained_tokenizer\", 'zip', \"trained_tokenizer\")\n","\n","from google.colab import files\n","\n","# Download the trained_model.zip file\n","files.download(\"trained_model.zip\")\n","\n","# Download the trained_tokenizer.zip file\n","files.download(\"trained_tokenizer.zip\")\n","\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"g63_Ofj7vPDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"6C6cjsk0Kh8E"},"execution_count":null,"outputs":[]}]}